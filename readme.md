# Code execution

The execise was implemented using the given skeleton, therefore to run the program, the standart arguments are used.

# Implementation

The attention mechanism was based in the class literature avaliable on (https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).

The implementation skeleton was based in the pytorch implementation avaliable on (https://github.com/lucidrains/vit-pytorch/tree/main/vit_pytorch), so this was the main inspiration for the code, especially with einops.

The original implementation was also used as an insight resource (https://github.com/IBM/CrossViT/tree/main/models)

# Changes
The main changes in the original code comes with the implementation of early stop, the usage of AdamW and a LR scheduler.

